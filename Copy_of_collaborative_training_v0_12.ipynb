{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of collaborative-training-v0.12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maexx393/hivemindcolab/blob/main/Copy_of_collaborative_training_v0_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQIcM112KfI"
      },
      "source": [
        "<center><img src=\"https://i.imgur.com/FHMoW3N.png\" width=360px><br><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Collaborative training <sup>v0.9 alpha</sup></b></center>\n",
        "\n",
        "\n",
        "This notebook will use local or colab GPU to help train ALBERT-large collaboratively. Your instance will compute gradients and exchange them with a bunch of volunteers around the world. We explain how it works at the bottom. But for now, please run all cells :)\n",
        "\n",
        "__Warning:__ this is a test run, it will be terminated by __23:59 21 april GMT+0__. By participating, you help us improve the training infrastructure!\n",
        "\n",
        "\n",
        "_Co-developed by [leshanbog@](https://github.com/leshanbog), [yhn112@](https://github.com/yhn112) and [foksly@](https://github.com/foksly) from YSDA, developers from [hivemind](https://github.com/learning-at-home/hivemind) and [huggingface](http://huggingface.co)_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUlL47uTK7DN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397d9728-4f40-4878-c6f6-fd4dde89b4ac"
      },
      "source": [
        "!pip install transformers datasets sentencepiece torch_optimizer==0.1.0\n",
        "!git clone https://github.com/learning-at-home/hivemind -b master\n",
        "!cd hivemind && pip install -e .\n",
        "!curl -L https://hivemind-data.s3.us-east-2.amazonaws.com/wikitext103.tar.gz | tar xzf -\n",
        "\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"GPU device not found. If running in colab, please retry in a few minutes.\"\n",
        "device_name = torch.cuda.get_device_name(0)\n",
        "if device_name.endswith('T4') or device_name.endswith('P100'):\n",
        "  microbatch_size = 4\n",
        "else:\n",
        "  microbatch_size = 1\n",
        "print(f\"Running with device {device_name}, local batch size = {microbatch_size}\")\n",
        "\n",
        "!ulimit -n 4096 && HIVEMIND_THREADS=256 python ./hivemind/examples/albert/run_trainer.py \\\n",
        " --client_mode --initial_peers 3.14.12.209:31337 --averaging_expiration 10 --statistics_expiration 120 \\\n",
        " --batch_size_lead 300 --per_device_train_batch_size {microbatch_size} --gradient_accumulation_steps 1 \\\n",
        " --logging_first_step --logging_steps 100  --output_dir ./outputs --overwrite_output_dir --logging_dir ./logs \\\n",
        " --experiment_prefix albert-wikitext-v12 --seed 42"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 46.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 41.4 MB/s \n",
            "\u001b[?25hCollecting torch_optimizer==0.1.0\n",
            "  Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer==0.1.0) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torch_optimizer==0.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: xxhash, tokenizers, sacremoses, pyyaml, pytorch-ranger, huggingface-hub, fsspec, transformers, torch-optimizer, sentencepiece, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed datasets-1.11.0 fsspec-2021.8.1 huggingface-hub-0.0.16 pytorch-ranger-0.1.1 pyyaml-5.4.1 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.10.3 torch-optimizer-0.1.0 transformers-4.10.2 xxhash-2.0.2\n",
            "Cloning into 'hivemind'...\n",
            "remote: Enumerating objects: 6268, done.\u001b[K\n",
            "remote: Counting objects: 100% (2875/2875), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1237/1237), done.\u001b[K\n",
            "remote: Total 6268 (delta 2006), reused 2310 (delta 1578), pack-reused 3393\u001b[K\n",
            "Receiving objects: 100% (6268/6268), 3.12 MiB | 9.12 MiB/s, done.\n",
            "Resolving deltas: 100% (4277/4277), done.\n",
            "Obtaining file:///content/hivemind\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: grpcio>=1.33.2 in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (1.39.0)\n",
            "Collecting grpcio-tools>=1.33.2\n",
            "  Using cached grpcio_tools-1.40.0-cp37-cp37m-manylinux2014_x86_64.whl (2.5 MB)\n",
            "Collecting uvloop>=0.14.0\n",
            "  Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting configargparse>=1.2.3\n",
            "  Downloading ConfigArgParse-1.5.2-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (1.19.5)\n",
            "Collecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prefetch-generator>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (1.9.0+cu102)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (3.17.3)\n",
            "Collecting multiaddr>=0.0.9\n",
            "  Downloading multiaddr-0.0.9-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pymultihash>=0.8.2\n",
            "  Downloading pymultihash-0.8.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (1.4.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from hivemind==1.0.0.dev0) (5.4.1)\n",
            "Collecting cryptography>=3.4.6\n",
            "  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.4.6->hivemind==1.0.0.dev0) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.4.6->hivemind==1.0.0.dev0) (2.20)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.33.2->hivemind==1.0.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from grpcio-tools>=1.33.2->hivemind==1.0.0.dev0) (57.4.0)\n",
            "Collecting grpcio>=1.33.2\n",
            "  Using cached grpcio-1.40.0-cp37-cp37m-manylinux2014_x86_64.whl (4.3 MB)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting varint\n",
            "  Downloading varint-1.0.2.tar.gz (1.9 kB)\n",
            "Collecting netaddr\n",
            "  Downloading netaddr-0.8.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8.1->hivemind==1.0.0.dev0) (3.7.4.3)\n",
            "Building wheels for collected packages: varint\n",
            "  Building wheel for varint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for varint: filename=varint-1.0.2-py3-none-any.whl size=1979 sha256=baae98df1250f9ed0944e4c3bb34ebdb066bf83fbfbaf09e1484286277f41547\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/21/07/09f1c6a7d9b59377aa6d98da6efdd670f7ca40aabd93d02704\n",
            "Successfully built varint\n",
            "Installing collected packages: varint, netaddr, grpcio, base58, uvloop, pymultihash, pydantic, multiaddr, grpcio-tools, cryptography, configargparse, hivemind\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.39.0\n",
            "    Uninstalling grpcio-1.39.0:\n",
            "      Successfully uninstalled grpcio-1.39.0\n",
            "  Running setup.py develop for hivemind\n",
            "Successfully installed base58-2.1.0 configargparse-1.5.2 cryptography-3.4.8 grpcio-1.40.0 grpcio-tools-1.40.0 hivemind-1.0.0.dev0 multiaddr-0.0.9 netaddr-0.8.0 pydantic-1.8.2 pymultihash-0.8.2 uvloop-0.16.0 varint-1.0.2\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  898M  100  898M    0     0  14.6M      0  0:01:01  0:01:01 --:--:-- 15.7M\n",
            "Running with device Tesla K80, local batch size = 1\n",
            "Sep 13 01:46:39.223 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1m__main__.main:208\u001b[0m] Found 1 initial peers: ['3.14.12.209:31337']\n",
            "Sep 13 01:46:39.224 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1m__main__.main:213\u001b[0m] Training/evaluation parameters:\n",
            "AlbertTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-06,\n",
            "clamp_value=10000.0,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=4,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O2,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.00176,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./logs,\n",
            "logging_first_step=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=125000,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=./outputs,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=outputs,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./outputs,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "seq_length=512,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=5000,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "Sep 13 01:46:39.986 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mfilelock.acquire:274\u001b[0m] Lock 140549998188304 acquired on data/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.044d033374cf383119201e83a30fc592581d33c3833ac0675f55b2765509ce4e.lock\n",
            "Sep 13 01:46:39.988 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.file_utils.get_from_cache:1665\u001b[0m] https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-config.json not found in cache or force_download set to True, downloading to /content/data/tmpmyi8z5b7\n",
            "Downloading: 100% 685/685 [00:00<00:00, 566kB/s]\n",
            "Sep 13 01:46:40.763 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.file_utils.get_from_cache:1669\u001b[0m] storing https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-config.json in cache at data/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.044d033374cf383119201e83a30fc592581d33c3833ac0675f55b2765509ce4e\n",
            "Sep 13 01:46:40.764 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.file_utils.get_from_cache:1677\u001b[0m] creating metadata file for data/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.044d033374cf383119201e83a30fc592581d33c3833ac0675f55b2765509ce4e\n",
            "Sep 13 01:46:40.764 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mfilelock.release:318\u001b[0m] Lock 140549998188304 released on data/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.044d033374cf383119201e83a30fc592581d33c3833ac0675f55b2765509ce4e.lock\n",
            "Sep 13 01:46:40.764 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.configuration_utils.get_config_dict:561\u001b[0m] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-config.json from cache at data/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.044d033374cf383119201e83a30fc592581d33c3833ac0675f55b2765509ce4e\n",
            "Sep 13 01:46:40.765 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.configuration_utils.from_dict:598\u001b[0m] Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Sep 13 01:46:40.765 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.tokenization_utils_base.from_pretrained:1669\u001b[0m] Didn't find file data/tokenizer/tokenizer.json. We won't load it.\n",
            "Sep 13 01:46:40.766 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.tokenization_utils_base.from_pretrained:1669\u001b[0m] Didn't find file data/tokenizer/added_tokens.json. We won't load it.\n",
            "Sep 13 01:46:40.766 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.tokenization_utils_base.from_pretrained:1737\u001b[0m] loading file data/tokenizer/spiece.model\n",
            "Sep 13 01:46:40.766 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.tokenization_utils_base.from_pretrained:1737\u001b[0m] loading file None\n",
            "Sep 13 01:46:40.766 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.tokenization_utils_base.from_pretrained:1737\u001b[0m] loading file None\n",
            "Sep 13 01:46:40.766 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.tokenization_utils_base.from_pretrained:1737\u001b[0m] loading file data/tokenizer/special_tokens_map.json\n",
            "Sep 13 01:46:40.766 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1mtransformers.tokenization_utils_base.from_pretrained:1737\u001b[0m] loading file data/tokenizer/tokenizer_config.json\n",
            "Sep 13 01:46:40.766 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mtransformers.configuration_utils.get_config_dict:538\u001b[0m] file data/tokenizer/config.json not found\n",
            "Sep 13 01:46:40.820 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mtransformers.configuration_utils.get_config_dict:538\u001b[0m] file data/tokenizer/config.json not found\n",
            "Sep 13 01:46:40.922 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1m__main__.get_model:41\u001b[0m] Checkpoint dir outputs, contents []\n",
            "Sep 13 01:46:40.922 [\u001b[1m\u001b[34mINFO\u001b[0m] [\u001b[1m__main__.get_model:48\u001b[0m] Training from scratch\n",
            "Traceback (most recent call last):\n",
            "  File \"./hivemind/examples/albert/run_trainer.py\", line 302, in <module>\n",
            "    main()\n",
            "  File \"./hivemind/examples/albert/run_trainer.py\", line 239, in main\n",
            "    identity_path=collaboration_args.identity_path,\n",
            "  File \"/content/hivemind/hivemind/dht/__init__.py\", line 100, in __init__\n",
            "    self.run_in_background(await_ready=await_ready)\n",
            "  File \"/content/hivemind/hivemind/dht/__init__.py\", line 157, in run_in_background\n",
            "    self.wait_until_ready(timeout)\n",
            "  File \"/content/hivemind/hivemind/dht/__init__.py\", line 160, in wait_until_ready\n",
            "    self._ready.result(timeout=timeout)\n",
            "  File \"/content/hivemind/hivemind/utils/mpfuture.py\", line 253, in result\n",
            "    return super().result(timeout)\n",
            "  File \"/usr/lib/python3.7/concurrent/futures/_base.py\", line 435, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\n",
            "    raise self._exception\n",
            "hivemind.p2p.p2p_daemon_bindings.control.P2PDaemonError: Daemon failed to start: 2021/09/13 01:46:50 failed to parse multiaddr \"3.14.12.209:31337\": must begin with /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93KCg3RZzEks"
      },
      "source": [
        "### What's up next?\n",
        "* Check the training progress on public learning curves: https://wandb.ai/yhn112/Demo-run/runs/1ygmigce\n",
        "* See [this tutorial](https://github.com/learning-at-home/hivemind/tree/master/examples/albert) on how to start your own collaborative runs!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUQ_j06kS6pb"
      },
      "source": [
        "### How it works\n",
        "\n",
        "Since peers can join and leave at any time, we can't use global [Ring All-Reduce](https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da) for averaging: a single missing peer can break the entire protocol. Instead, peers dynamically assemble into small groups and run all-reduce within each group. Consider an example with 9 GPUs:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/QcD1mfG.png\" width=360px><br>\n",
        "The All-Reduce protocol within group can be Ring-AllReduce, but we use a simpler all-to-all algorithm known as butterfly-like all-reduce.<br>\n",
        "<img src=\"https://i.imgur.com/ewq3vS6.png\" width=380px><br>\n",
        "After each successful round, participants shuffle around and find new groups:<br>\n",
        "<img src=\"https://i.imgur.com/dexNCL3.png\" width=350px>\n",
        "\n",
        "If one of the peers fails to do his part, it will only affect his local group, and only for a single round.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/RBmElUi.png\" width=340px>\n",
        "\n",
        "Afterwards, peers from the failed group will find new groupmates according to the [moshpit algorithm](https://arxiv.org/abs/2103.03239).\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "If you want to learn more and even host your own collaborative experiments, take a look at the [hivemind library](https://github.com/learning-at-home/hivemind/) or the [Moshpit-SGD paper](https://arxiv.org/pdf/2103.03239.pdf).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJAHgQMPWBeP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}